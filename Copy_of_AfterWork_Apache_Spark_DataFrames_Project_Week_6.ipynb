{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Importation and Exploration"
      ],
      "metadata": {
        "id": "AuWWQg_38z_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing pyspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "zo_dK6s6t5wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import format_number, year, month, dayofmonth, max, corr\n"
      ],
      "metadata": {
        "id": "LE55Nw3b9oyV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start a Spark session\n",
        "spark = SparkSession.builder.appName('SafaricomStockAnalysis').getOrCreate()\n",
        "\n",
        "# Load the stock file while inferring the data types\n",
        "df = spark.read.csv('/content/saf_stock (1).csv', header=True, inferSchema=True)\n",
        "\n",
        "# Determine the column names\n",
        "print(df.columns)\n",
        "\n",
        "# Observations about the schema\n",
        "df.printSchema()\n",
        "\n",
        "# Show the first 5 rows\n",
        "df.show(5)\n",
        "\n",
        "# Use the describe method to learn about the data frame\n",
        "df.describe().show()"
      ],
      "metadata": {
        "id": "OzGplM5N9xpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "PK1E0xG0Kvj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format all the data to 2 decimal places\n",
        "df = df.select([format_number(col, 2).alias(col) if col != 'Date' else col for col in df.columns])\n",
        "\n",
        "# Create a new data frame with a column called HV Ratio\n",
        "df2 = df.withColumn('HV Ratio', format_number(df['High']/df['Volume'], 2))\n",
        "df2.show(5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eg7JKbNsqG6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Analysis\n"
      ],
      "metadata": {
        "id": "mSG-eivkLFrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "# Find the day with the Peak High Price\n",
        "max_high = df2.select(max('High')).collect()[0][0]\n",
        "peak_high_day = df2.filter(df2['High'] == max_high).select('Date').collect()[0][0]\n",
        "print('The day with the Peak High Price is:', peak_high_day)\n",
        "\n"
      ],
      "metadata": {
        "id": "2g1pnMvDqjpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Find the mean of the Close column\n",
        "mean_close = df2.select(mean('Close')).collect()[0][0]\n",
        "print('The mean of the Close column is:', mean_close)\n",
        "\n"
      ],
      "metadata": {
        "id": "yxGIeHr3_7nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, min\n",
        "\n",
        "# Find the max and min of the Volume column\n",
        "max_volume = df2.select(max('Volume')).collect()[0][0]\n",
        "min_volume = df2.select(min('Volume')).collect()[0][0]\n",
        "print('The maximum volume traded is:', max_volume)\n",
        "print('The minimum volume traded is:', min_volume)\n",
        "\n"
      ],
      "metadata": {
        "id": "vKsSNKj_AET-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Find the number of days when Close was lower than 60 dollars\n",
        "days_close_below_60 = df2.filter(df2['Close'] < 60).select(count('Date')).collect()[0][0]\n",
        "print('The number of days when Close was lower than 60 dollars is:', days_close_below_60)\n",
        "\n"
      ],
      "metadata": {
        "id": "vJvimOu7BhxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when\n",
        "\n",
        "# Calculate the percentage of time when High was greater than 80 dollars\n",
        "total_days = df2.select(count('Date')).collect()[0][0]\n",
        "high_greater_80_days = df2.filter(df2['High'] > 80).select(count('Date')).collect()[0][0]\n",
        "high_greater_80_percentage = high_greater_80_days / total_days * 100\n",
        "print('The percentage of time when High was greater than 80 dollars is:', round(high_greater_80_percentage, 2), '%')\n",
        "\n"
      ],
      "metadata": {
        "id": "vZuDAhqcBrZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr\n",
        "\n",
        "# Find the Pearson correlation between High and Volume\n",
        "corr_high_volume = df2.select(corr('High', 'Volume')).collect()[0][0]\n",
        "\n",
        "if corr_high_volume is not None:\n",
        "    print(\"The Pearson correlation between High and Volume is: {:.2f}\".format(corr_high_volume))\n",
        "else:\n",
        "    print(\"There are not enough non-null observations to calculate the correlation coefficient.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FoOmpBk-B6uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the max High per year\n",
        "from pyspark.sql.functions import year, max\n",
        "\n",
        "# Find the max High per year\n",
        "max_high_year = df2.groupBy(year('Date')).agg(max('High').alias('Max High')).orderBy('year(Date)')\n",
        "max_high_year.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vO66hHUWFizw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average Close for each calendar month\n",
        "from pyspark.sql.functions import month, avg\n",
        "\n",
        "# Find the average Close for each Calendar Month\n",
        "avg_close_month = df2.groupBy(month('Date')).agg(avg('Close').alias('Avg Close')).orderBy('month(Date)')\n",
        "avg_close_month.show()\n"
      ],
      "metadata": {
        "id": "M17LoRcRF7NO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}